\documentclass{article}
\linespread{1.3}
\usepackage[margin=50pt]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsthm, tikz, fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\newcommand{\changefont}{\fontsize{15}{15}\selectfont}

\fancypagestyle{firstpageheader}
{
  \fancyhead[R]{\changefont Michael Huang \\ Homework 8 \\ Adekoya}
}

\begin{document}

\thispagestyle{firstpageheader}

\section*{1.}
{\Large 
Choose a number $X$ at random from the set of numbers
$\left\{ 1,2,3,4,5\right\} $. Now choose a number $Y$ at random from the set $\left\{ 1,\dots,X\right\} $. 

\subsection*{(a)}
Find the joint pmf of $X$ and $Y$. Hint: think of how  joint pmf  is connected to conditional pmf of $Y$ given $X$. \\ \\
We aim to find $P(X = x, Y = y)$. Because of the way that $Y$ relies on $X$, we can use the definition of conditional probability to simplify this to $P(X = x, Y = y) = P(Y = y \mid X = x)P(X = x) = \frac{1}{x} \cdot \frac{1}{5} = $ \framebox[1.1\width]{\textbf{$\frac{1}{5x}$}}, since the distribution of $X$ is a uniform one across 5 numbers, and $P(Y = y)$ is determined by this initial choice of $x$. We note that this only holds when $x \leq y$, by definition, and is 0 elsewhere.

\subsection*{(b)}
Find the conditional mass function of $X$ given that $Y=i$,
for $i=1,2,3,4,5$. That is, find the pmf of $X|{Y=i}$. \\ \\ 
We aim to find $P(X = x \mid Y = i)$ for $i = 1,2,3,4,5$. We know by the definition of conditional probability that $P(X = x \mid Y = i) = \frac{P(X = x, Y = i)}{P(Y = i)}$. \\
We found $P(X = x, Y = i)$ previously in part (a). To find $P(Y = i)$, we need to find $P(X = x, Y = i)$ for each possible $x$. For each possible value of $x$ that we derived $i$ from, we know that $P(X = x, Y = i) = P(Y = i \mid X = x)P(X = x) = \frac{1}{5x}$. To account for all the possible cases of $x$ from which we derived our $i$, however, we need to take the sum of the possible $x \geq i$ using the law of total probability, giving us that $P(Y = i) = \sum_{x = i}^{5} \frac{1}{5x}$. \\ \\ 
Putting it all together, we find that $P(X = x \mid Y = i) = \frac{P(X = x, Y = i)}{P(Y = i)} = $ \framebox[1.1\width]{\textbf{$\frac{\frac{1}{5x}}{\sum_{x=i}^{5} \frac{1}{5x}}$}}, which we note is only valid for $x \geq i$.

\subsection*{(c)}
Are $X$ and $Y$ independent? \\ \\ 
We will show that $X$ and $Y$ are not independent by contradiction. Suppose that $X$ and $Y$ are independent. By definition, this would mean that $P(X = x) \cdot P(Y = y) = P(X = x, Y = y)$. Take for example $x = 1$ and $y = 2$. We would therefore have that $P(X = 1) \cdot P(Y = 2) = P(X = 1, Y = 2)$. We note that $P(X = 1)$ and $P(Y = 2)$ are all probabilities with valid values and therefore nonzero probabilities. However, $P(X = 1, Y = 2)$ is clearly not possible by definition, since we must have $y \leq x$, so its probability is 0. Since we have two nonzero probabilities multiplying to result in a probability of zero, this is a contradiction. Therefore, $X$ and $Y$ are \framebox[1.1\width]{\textbf{not independent.}}

}

\section*{2.}
{\Large

The county hospital is located at the center of a square
whose sides are $3$ miles wide. If an accident occurs within this
square, then the hospital sends out an ambulance. The road network
is rectangular, so the travel distance from the hospital, whose coordinates
are $(0,0)$, to the point $(x,y)$ is $|x|+|y|$. If an accident
occurs at a point that is uniformly distributed in the square, find
the expected travel distance of the ambulance. \\ \\ 
We aim to find the expected distance, or $E(|X| + |Y|)$, where $f_{X, Y} = \frac{1}{9}$ for $-\frac{3}{2} \leq x,y \leq \frac{3}{2}$, since our square centered at (0, 0) is 3 miles long on each side for a total of 9 square miles, and $F_{X, Y} = 1$, so we can find that $f_x = c = \frac{1}{9}$ (alternatively, solving the equation $F_X = 1 = \int_{-\frac{3}{2}}^{\frac{3}{2}} \int_{-\frac{3}{2}}^{\frac{3}{2}} c \,dx \,dy $ also gives us $c = \frac{1}{9}$). We can then take $E(|X| + |Y|)$ as follows: \\
$E(|X| + |Y|) = \int \int (|x| + |y|) \cdot f_{X, Y} \,dx \,dy = \int \int (|x| + |y|) \cdot c \,dx \,dy = c \cdot \int \int (|x| + |y|) \,dx \,dy$ \\
$ = \frac{1}{9} \cdot \int_{-\frac{3}{2}}^{\frac{3}{2}} \int_{-\frac{3}{2}}^{\frac{3}{2}} (|x| + |y|) \,dx \,dy$ \\ 
$ = \frac{1}{9} \cdot \int_{-\frac{3}{2}}^{\frac{3}{2}} \int_{-\frac{3}{2}}^{\frac{3}{2}} |x| \,dy \,dx + \frac{1}{9} \cdot \int_{-\frac{3}{2}}^{\frac{3}{2}} \int_{-\frac{3}{2}}^{\frac{3}{2}} |y| \,dx \,dy$ \\ 
$ = \frac{1}{9} \cdot \int_{-\frac{3}{2}}^{\frac{3}{2}} (|x|y) |_{-\frac{3}{2}}^{\frac{3}{2}} \,dx + \frac{1}{9} \cdot \int_{-\frac{3}{2}}^{\frac{3}{2}} (|y|x) |_{-\frac{3}{2}}^{\frac{3}{2}} \,dy$ \\ 
$ = \frac{1}{3} \cdot \int_{-\frac{3}{2}}^{\frac{3}{2}} |x| \,dx + \frac{1}{3} \cdot \int_{-\frac{3}{2}}^{\frac{3}{2}} |y| \,dy$, which we can combine using change of variable to be \\ 
$ = \frac{2}{3} \cdot \int_{-\frac{3}{2}}^{\frac{3}{2}} |x| \,dx$ \\ 
$ = \frac{2}{3} \cdot 2 \cdot \int_{0}^{\frac{3}{2}} x \,dx$ \\
$ = \frac{4}{3} \cdot \frac{x^2}{2} |_{0}^{\frac{3}{2}}$ \\
$ = \frac{4}{3} \cdot \frac{9}{4}$ \\
$ = $ \framebox[1.1\width]{\textbf{$\frac{3}{2}$}} \\

}

\section*{3.}
{\Large 

There are 3 distinct types of coupons. Independently
of past types collected, each new one is obtained with the following probabilities: $1/2$ for type $1$, $1/3$ for type $2$, and $1/6$ for type $3$. Let $X$ be the number of different types of coupons that appear among the first $n$ collected. Find the expected value and variance of $X$. \\ \\
We know that for every round we collect a coupon, we have a probability of collecting each coupon as stated above. We also know that the types of coupons collected are independent from one another. Therefore, we can say that for each coupon type $i$, the probability that we will not have the coupon after $n$ many coupon collections, or $P(C'_i)$ where $C'$ represents not collecting the coupon, is simply $P(C'_i) = (1-P(C_i))^n$. Each $P(C'_i)$ represents the probability of not collecting coupon $i$ at some stage, so we simply aim to take the complement of this to find the probability of collecting the coupon at some stage, and then summing all of these distinct probabilities together to find the expected value of the number of distinct number of coupons that we will collect through $n$ many coupon collections. Mathematically, we can take this to be $\sum_{i = 1}^{3} 1 - P(C'_i) = 3 + \sum_{i=1}^{3} - P(C'_i) = 3 - \sum_{i=1}^{3} (1 - P(C_i))^n = $ \framebox[1.1\width]{\textbf{$3 - (\frac{1}{2})^n - (\frac{2}{3})^n - (\frac{5}{6})^n$}} \\ \\ 
If we consider $Y$ to be the geometric variables of $X$, in the way that $X = 3 - Y$, then we know by definition of variance that $Var(X) = Var(Y)$. We can then look at the those $Y$ elements specifically, i.e. $(\frac{1}{2})^n + (\frac{2}{3})^n + (\frac{5}{6})^n$. By variance of sums and the fact that $Y_1, Y_2, Y_3$ are independent by definition, we can take the variance $Var(Y_1 + Y_2 + Y_3) = Var(Y_1) + Var(Y_2) + Var(Y_3) = \frac{1-\frac{1}{2}}{(\frac{1}{2})^2} + \frac{1-\frac{2}{3}}{(\frac{2}{3})^2} + \frac{1-\frac{5}{6}}{(\frac{5}{6})^2} = 2 + \frac{3}{4} + \frac{6}{25} = \frac{299}{100} = $ \framebox[1.1\width]{\textbf{2.99}}, by definition of variance for random geometric variable.

}

\section*{4.}
{\Large 
The following are general questions about covariance and
correlation.

\subsection*{(a)}
Let $X\sim{\rm Bin}\left(n,p\right)$ and $Y\sim G\left(q\right)$
be random variables ($p,q\in\left(0,1\right)$). Is it possible that $\left|\rho_{X,Y}\right|=1$? \\ \\ 
We know that $|\rho_{X, Y}| = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}$. Yes because geometric is related to bernoulli which is the basis for binomial. However, they do not look the same in terms of their actual distribution and differe in terms of finite/infinite values. 

\subsection*{(b)}
Let $X$ be a random variable. Is it possible that $\rho_{X,X^{2}}=1$ for some $X$? What about $\rho_{X,X^{2}}=-1$? \\ \\ 
It is possible that $\rho_{X, X^2} = 1$; say, for example, if $X$ is the Bernoulli distribution, then $X = X^2$. \\ 
It is possible that $\rho_{X, X^2} = -1$. For example, if $X$ is the Bernoulli distribution but with negative values as outcome indicators, then we end up with $X$ as being negative, and $X^2 = (-X^2)$ except positive, which would lead to a correlation coefficient of -1. \\ 
% Can the Bernoulli be negative?

\subsection*{(c)}
Let $X,Y$ be random variables with the following joint
distribution (with $R>0$)
\[
f_{X,Y}\left(x,y\right)=\begin{cases}
c & \,x^{2}+y^{2}\le R;\\
0 & \,\mbox{otherwise}.
\end{cases}
\]
Show that $X,Y$ are uncorrelated, but that they are not independent. \\ \\
We know that $|\rho_{X, Y}| = \frac{Cov(X, Y)}{\sqrt{Var(X)Var(Y)}}$. \\ 
We know that $Cov(X, Y) = E(XY) - E(X)E(Y)$. We know that for this kind of uniform distribution, $E(XY) = \int \int xy \cdot f_{X, Y} \,dx \,dy = \int \int xy \cdot c \,dx \,dy = c \cdot \int \int xy \,dx \,dy$ \\ 
$ = c \cdot \int_{-\sqrt{R}}^{\sqrt{R}} \int_{-\sqrt{R}}^{\sqrt{R}} xy \,dx \,dy =$ \\ 
$ c \cdot \int_{-\sqrt{R}}^{\sqrt{R}} y (\frac{x^2}{2} |_{-\sqrt{R}}^{\sqrt{R}}) \,dy$ \\ 
$ = c \cdot \int_{-\sqrt{R}}^{\sqrt{R}} \frac{R}{2} - \frac{R}{2} \cdot y \,dy$ \\
$ = c \cdot \int_{-\sqrt{R}}^{\sqrt{R}} 0 \cdot y \,dy$ \\
$ = c \cdot \int_{-\sqrt{R}}^{\sqrt{R}} 0\,dy$ \\
$ = 0$ \\ \\ 
In the same way: $E(X) = \int \int x \cdot f_{X, Y} \,dx \,dy = $ \\ 
$= \int_{-\sqrt{R}}^{\sqrt{R}} \frac{x^2}{2} |_{-\sqrt{R}}^{\sqrt{R}} \,dy $ \\ 
$= \int_{-\sqrt{R}}^{\sqrt{R}} \frac{R}{2} - \frac{R}{2} \,dy $ \\ 
$= \int_{-\sqrt{R}}^{\sqrt{R}} 0 \,dy $ \\ 
$= 0$ \\ 
We find that $E(X)$ = 0, which tells us that $E(X)E(Y) = 0$ as well. In addition, since we found that $E(XY) = 0$, then $Cov(X,Y) = E(XY) - E(X)E(Y) = 0 - 0 = 0$, which in turn tells us that $|\rho_{X, Y}| = \frac{Cov(X, Y)}{\sqrt{Var(X)Var(Y)}} = \frac{0}{\sqrt{Var(X)Var(Y)}} = 0$. This allows us to conclude that $X$ and $Y$ are not correlated. \\ \\
Now we must show that $X$ and $Y$ are not independent. We will show this by contradiction. Suppose $X$ and $Y$ are independent. This would mean that $P(X = a) \cdot P(Y = b) = P(X = a \cap Y = b)$. However, take the examples $a = \sqrt{R}$ and $b = \sqrt{R}$. In this case, we would expect that if  $X$ and $Y$ are independent, $P(X = \sqrt{R}) \cdot P(Y = \sqrt{R}) = P(X = \sqrt{R} \cap Y = \sqrt{R})$. However, note that while $P(X = \sqrt{R})$ and $P(Y = \sqrt{R})$ separately are nonzero according to the range of $X$ and $Y$ that we can derive from the joint pdf, $P(X = \sqrt{R} \cap Y = \sqrt{R})$ is zero since this would lead to $X^2 + Y^2 = \sqrt{R}^2 + \sqrt{R}^2 = R + R > 2R$, which is out of the bounds of our joint pdf. This is a contradiction. Therefore, we can conclude that $X$ and $Y$ are not independent.

}

\end{document}