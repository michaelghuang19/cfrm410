\documentclass{article}
\linespread{1.3}
\usepackage[margin=50pt]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsthm, tikz, fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\newcommand{\changefont}{\fontsize{15}{15}\selectfont}

\fancypagestyle{firstpageheader}
{
  \fancyhead[R]{\changefont Michael Huang \\ Homework 8 \\ Adekoya}
}

\begin{document}

\thispagestyle{firstpageheader}

\section*{1.}
{\Large 
Choose a number $X$ at random from the set of numbers
$\left\{ 1,2,3,4,5\right\} $. Now choose a number $Y$ at random
from the set $\left\{ 1,\dots,X\right\} $.

\subsection*{(a)}
Find the joint pmf of $X$ and $Y$. Hint: think of how  joint pmf  is connected to conditional pmf of $Y$ given $X$.

\subsection*{(b)}
Find the conditional mass function of $X$ given that $Y=i$,
for $i=1,2,3,4,5$. That is, find the pmf of $X|_{Y=i}$. 

\subsection*{(c)}
Are $X$ and $Y$ independent?

}

\section*{2.}
{\Large

The county hospital is located at the center of a square
whose sides are $3$ miles wide. If an accident occurs within this
square, then the hospital sends out an ambulance. The road network
is rectangular, so the travel distance from the hospital, whose coordinates
are $(0,0)$, to the point $(x,y)$ is $|x|+|y|$. If an accident
occurs at a point that is uniformly distributed in the square, find
the expected travel distance of the ambulance.

}

\section*{3.}
{\Large 

There are 3 distinct types of coupons. Independently
of past types collected, each new one is obtained with the following probabilities: $1/2$ for type $1$, $1/3$ for type $2$, and $1/6$ for type $3$. Let $X$ be the number of different types of coupons that appear among the first $n$ collected. Find the expected value and variance of $X$. \\ \\
We know that for every round we collect a coupon, we have a probability of collecting each coupon as stated above. We also know that the types of coupons collected are independent from one another. Therefore, we can say that for each coupon type $i$, the probability that we will not have the coupon after $n$ many coupon collections, or $P(C'_i)$ where $C'$ represents not collecting the coupon, is simply $P(C'_i) = (1-P(C_i))^n$. Each $P(C'_i)$ represents the probability of not collecting coupon $i$ at some stage, so we simply aim to take the complement of this to find the probability of collecting the coupon at some stage, and then summing all of these distinct probabilities together to find the expected value of the number of distinct number of coupons that we will collect through $n$ many coupon collections. Mathematically, we can take this to be $\sum_{i = 1}^{3} 1 - P(C'_i) = 3 + \sum_{i=1}^{3} - P(C'_i) = 3 - \sum_{i=1}^{3} (1 - P(C_i))^n = $ \framebox[1.1\width]{\textbf{$3 - (\frac{1}{2})^n - (\frac{2}{3})^n - (\frac{5}{6})^n$}} \\ \\ 
If we consider $Y$ to be the geometric variables of $X$, in the way that $X = 3 - Y$, then we know by definition of variance that $Var(X) = Var(Y)$. We can then look at the those $Y$ elements specifically, i.e. $(\frac{1}{2})^n + (\frac{2}{3})^n + (\frac{5}{6})^n$. By variance of sums and the fact that $Y_1, Y_2, Y_3$ are independent by definition, we can take the variance $Var(Y_1 + Y_2 + Y_3) = Var(Y_1) + Var(Y_2) + Var(Y_3) = \frac{1-\frac{1}{2}}{(\frac{1}{2})^2} + \frac{1-\frac{2}{3}}{(\frac{2}{3})^2} + \frac{1-\frac{5}{6}}{(\frac{5}{6})^2} = 2 + \frac{3}{4} + \frac{6}{25} = \frac{299}{100} = $ \framebox[1.1\width]{\textbf{2.99}}, by definition of variance for random geometric variable.

}

\section*{4.}
{\Large 
The following are general questions about covariance and
correlation.

\subsection*{(a)}
Let $X\sim{\rm Bin}\left(n,p\right)$ and $Y\sim G\left(q\right)$
be random variables ($p,q\in\left(0,1\right)$). Is it possible that $\left|\rho_{X,Y}\right|=1$? \\ \\ 
We know that $|\rho_{X, Y}| = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}$. Yes because geometric is related to bernoulli which is the basis for binomial. However, they do not look the same in terms of their actual distribution and differe in terms of finite/infinite values. 

\subsection*{(b)}
Let $X$ be a random variable. Is it possible that $\rho_{X,X^{2}}=1$ for some $X$? What about $\rho_{X,X^{2}}=-1$? \\ \\ 
It is possible that $\rho_{X, X^2} = 1$; say, for example, if $X$ is the Bernoulli distribution, then $X = X^2$. \\ 
It is possible that $\rho_{X, X^2} = -1$. For example, if $X$ is the Bernoulli distribution but with negative values as outcome indicators, then we end up with $X$ as being negative, and $X^2 = (-X^2)$ except positive, which would lead to a correlation coefficient of -1. \\ 
% Can the Bernoulli be negative?

\subsection*{(c)}
Let $X,Y$ be random variables with the following joint
distribution (with $R>0$)
\[
f_{X,Y}\left(x,y\right)=\begin{cases}
c & \,x^{2}+y^{2}\le R;\\
0 & \,\mbox{otherwise}.
\end{cases}
\]
Show that $X,Y$ are uncorrelated, but that they are not independent. \\ \\
We know that $|\rho_{X, Y}| = \frac{Cov(X, Y)}{\sqrt{Var(X)Var(Y)}}$. \\ 
We know that $Cov(X, Y) = E(XY) - E(X)E(Y)$. We know that for this kind of uniform distribution, $E(XY) = \int \int xy \cdot f_{X, Y} \,dx \,dy = \int \int xy \cdot c \,dx \,dy = c \cdot \int \int xy \,dx \,dy$ \\ 
$ = c \cdot \int_{-\sqrt{R}}^{\sqrt{R}} \int_{-\sqrt{R}}^{\sqrt{R}} xy \,dx \,dy =$ \\ 
$ c \cdot \int_{-\sqrt{R}}^{\sqrt{R}} y (\frac{x^2}{2} |_{-\sqrt{R}}^{\sqrt{R}}) \,dy$ \\ 
$ = c \cdot \int_{-\sqrt{R}}^{\sqrt{R}} \frac{R}{2} - \frac{R}{2} \cdot y \,dy$ \\
$ = c \cdot \int_{-\sqrt{R}}^{\sqrt{R}} 0 \cdot y \,dy$ \\
$ = c \cdot \int_{-\sqrt{R}}^{\sqrt{R}} 0\,dy$ \\
$ = 0$ \\ \\ 
In the same way: $E(X) = \int \int x \cdot f_{X, Y} \,dx \,dy = $ \\ 
$= \int_{-\sqrt{R}}^{\sqrt{R}} \frac{x^2}{2} |_{-\sqrt{R}}^{\sqrt{R}} \,dy $ \\ 
$= \int_{-\sqrt{R}}^{\sqrt{R}} \frac{R}{2} - \frac{R}{2} \,dy $ \\ 
$= \int_{-\sqrt{R}}^{\sqrt{R}} 0 \,dy $ \\ 
$= 0$ \\ 
We find that $E(X)$ = 0, which tells us that $E(X)E(Y) = 0$ as well. In addition, since we found that $E(XY) = 0$, then $Cov(X,Y) = E(XY) - E(X)E(Y) = 0 - 0 = 0$, which in turn tells us that $|\rho_{X, Y}| = \frac{Cov(X, Y)}{\sqrt{Var(X)Var(Y)}} = \frac{0}{\sqrt{Var(X)Var(Y)}} = 0$. This allows us to conclude that $X$ and $Y$ are not correlated. \\ \\
Now we must show that $X$ and $Y$ are not independent. We will show this by contradiction. Suppose $X$ and $Y$ are independent. This would mean that $P(X = a) \cdot P(Y = b) = P(X = a \cap Y = b)$. However, take the examples $a = \sqrt{R}$ and $b = \sqrt{R}$. In this case, we would expect that if  $X$ and $Y$ are independent, $P(X = \sqrt{R}) \cdot P(Y = \sqrt{R}) = P(X = \sqrt{R} \cap Y = \sqrt{R})$. However, note that while $P(X = \sqrt{R})$ and $P(Y = \sqrt{R})$ separately are nonzero according to the range of $X$ and $Y$ that we can derive from the joint pdf, $P(X = \sqrt{R} \cap Y = \sqrt{R})$ is zero since this would lead to $X^2 + Y^2 = \sqrt{R}^2 + \sqrt{R}^2 = R + R > 2R$, which is out of the bounds of our joint pdf. This is a contradiction. Therefore, we can conclude that $X$ and $Y$ are not independent.

}

\end{document}